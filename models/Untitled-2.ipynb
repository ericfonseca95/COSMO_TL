{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'COSMO_TL'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mCOSMO_TL\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mctl\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'COSMO_TL'"
     ]
    }
   ],
   "source": [
    "import COSMO_TL as ctl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: c:/Users/efons/anaconda3/envs/venv/lib/site-packages/cosmo_tl-0.0.1-py3.10.egg/COSMO_TL/data/new_data_parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\efons\\anaconda3\\envs\\venv\\lib\\site-packages\\dask\\backends.py:125\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 125\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    126\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\efons\\anaconda3\\envs\\venv\\lib\\site-packages\\dask\\dataframe\\io\\parquet\\core.py:479\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, columns, filters, categories, index, storage_options, engine, use_nullable_dtypes, calculate_divisions, ignore_metadata_file, metadata_task_size, split_row_groups, chunksize, aggregate_files, parquet_file_extension, **kwargs)\u001b[0m\n\u001b[0;32m    477\u001b[0m     index \u001b[39m=\u001b[39m [index]\n\u001b[1;32m--> 479\u001b[0m read_metadata_result \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39mread_metadata(\n\u001b[0;32m    480\u001b[0m     fs,\n\u001b[0;32m    481\u001b[0m     paths,\n\u001b[0;32m    482\u001b[0m     categories\u001b[39m=\u001b[39mcategories,\n\u001b[0;32m    483\u001b[0m     index\u001b[39m=\u001b[39mindex,\n\u001b[0;32m    484\u001b[0m     use_nullable_dtypes\u001b[39m=\u001b[39muse_nullable_dtypes,\n\u001b[0;32m    485\u001b[0m     gather_statistics\u001b[39m=\u001b[39mcalculate_divisions,\n\u001b[0;32m    486\u001b[0m     filters\u001b[39m=\u001b[39mfilters,\n\u001b[0;32m    487\u001b[0m     split_row_groups\u001b[39m=\u001b[39msplit_row_groups,\n\u001b[0;32m    488\u001b[0m     chunksize\u001b[39m=\u001b[39mchunksize,\n\u001b[0;32m    489\u001b[0m     aggregate_files\u001b[39m=\u001b[39maggregate_files,\n\u001b[0;32m    490\u001b[0m     ignore_metadata_file\u001b[39m=\u001b[39mignore_metadata_file,\n\u001b[0;32m    491\u001b[0m     metadata_task_size\u001b[39m=\u001b[39mmetadata_task_size,\n\u001b[0;32m    492\u001b[0m     parquet_file_extension\u001b[39m=\u001b[39mparquet_file_extension,\n\u001b[0;32m    493\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    494\u001b[0m )\n\u001b[0;32m    496\u001b[0m \u001b[39m# In the future, we may want to give the engine the\u001b[39;00m\n\u001b[0;32m    497\u001b[0m \u001b[39m# option to return a dedicated element for `common_kwargs`.\u001b[39;00m\n\u001b[0;32m    498\u001b[0m \u001b[39m# However, to avoid breaking the API, we just embed this\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[39m# data in the first element of `parts` for now.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m \u001b[39m# The logic below is inteded to handle backward and forward\u001b[39;00m\n\u001b[0;32m    501\u001b[0m \u001b[39m# compatibility with a user-defined engine.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\efons\\anaconda3\\envs\\venv\\lib\\site-packages\\dask\\dataframe\\io\\parquet\\arrow.py:355\u001b[0m, in \u001b[0;36mArrowDatasetEngine.read_metadata\u001b[1;34m(cls, fs, paths, categories, index, use_nullable_dtypes, gather_statistics, filters, split_row_groups, chunksize, aggregate_files, ignore_metadata_file, metadata_task_size, parquet_file_extension, **kwargs)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    336\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_metadata\u001b[39m(\n\u001b[0;32m    337\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    353\u001b[0m \n\u001b[0;32m    354\u001b[0m     \u001b[39m# Stage 1: Collect general dataset information\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m     dataset_info \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_collect_dataset_info(\n\u001b[0;32m    356\u001b[0m         paths,\n\u001b[0;32m    357\u001b[0m         fs,\n\u001b[0;32m    358\u001b[0m         categories,\n\u001b[0;32m    359\u001b[0m         index,\n\u001b[0;32m    360\u001b[0m         gather_statistics,\n\u001b[0;32m    361\u001b[0m         filters,\n\u001b[0;32m    362\u001b[0m         split_row_groups,\n\u001b[0;32m    363\u001b[0m         chunksize,\n\u001b[0;32m    364\u001b[0m         aggregate_files,\n\u001b[0;32m    365\u001b[0m         ignore_metadata_file,\n\u001b[0;32m    366\u001b[0m         metadata_task_size,\n\u001b[0;32m    367\u001b[0m         parquet_file_extension,\n\u001b[0;32m    368\u001b[0m         kwargs,\n\u001b[0;32m    369\u001b[0m     )\n\u001b[0;32m    371\u001b[0m     \u001b[39m# Stage 2: Generate output `meta`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\efons\\anaconda3\\envs\\venv\\lib\\site-packages\\dask\\dataframe\\io\\parquet\\arrow.py:874\u001b[0m, in \u001b[0;36mArrowDatasetEngine._collect_dataset_info\u001b[1;34m(cls, paths, fs, categories, index, gather_statistics, filters, split_row_groups, chunksize, aggregate_files, ignore_metadata_file, metadata_task_size, parquet_file_extension, kwargs)\u001b[0m\n\u001b[0;32m    873\u001b[0m \u001b[39mif\u001b[39;00m ds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 874\u001b[0m     ds \u001b[39m=\u001b[39m pa_ds\u001b[39m.\u001b[39mdataset(\n\u001b[0;32m    875\u001b[0m         paths,\n\u001b[0;32m    876\u001b[0m         filesystem\u001b[39m=\u001b[39mfs,\n\u001b[0;32m    877\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_dataset_kwargs,\n\u001b[0;32m    878\u001b[0m     )\n\u001b[0;32m    880\u001b[0m \u001b[39m# Deal with directory partitioning\u001b[39;00m\n\u001b[0;32m    881\u001b[0m \u001b[39m# Get all partition keys (without filters) to populate partition_obj\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\efons\\anaconda3\\envs\\venv\\lib\\site-packages\\pyarrow\\dataset.py:755\u001b[0m, in \u001b[0;36mdataset\u001b[1;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[0;32m    754\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(_is_path_like(elem) \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m source):\n\u001b[1;32m--> 755\u001b[0m     \u001b[39mreturn\u001b[39;00m _filesystem_dataset(source, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    756\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(elem, Dataset) \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m source):\n",
      "File \u001b[1;32mc:\\Users\\efons\\anaconda3\\envs\\venv\\lib\\site-packages\\pyarrow\\dataset.py:442\u001b[0m, in \u001b[0;36m_filesystem_dataset\u001b[1;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(source, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m--> 442\u001b[0m     fs, paths_or_selector \u001b[39m=\u001b[39m _ensure_multiple_sources(source, filesystem)\n\u001b[0;32m    443\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\efons\\anaconda3\\envs\\venv\\lib\\site-packages\\pyarrow\\dataset.py:361\u001b[0m, in \u001b[0;36m_ensure_multiple_sources\u001b[1;34m(paths, filesystem)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[39melif\u001b[39;00m file_type \u001b[39m==\u001b[39m FileType\u001b[39m.\u001b[39mNotFound:\n\u001b[1;32m--> 361\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(info\u001b[39m.\u001b[39mpath)\n\u001b[0;32m    362\u001b[0m \u001b[39melif\u001b[39;00m file_type \u001b[39m==\u001b[39m FileType\u001b[39m.\u001b[39mDirectory:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: c:/Users/efons/anaconda3/envs/venv/lib/site-packages/cosmo_tl-0.0.1-py3.10.egg/COSMO_TL/data/new_data_parquet",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ctl\u001b[39m.\u001b[39;49mGammaLoaders\u001b[39m.\u001b[39;49mget_IDAC_df()\n",
      "File \u001b[1;32mc:\\Users\\efons\\anaconda3\\envs\\venv\\lib\\site-packages\\cosmo_tl-0.0.1-py3.10.egg\\COSMO_TL\\GammaLoaders.py:372\u001b[0m, in \u001b[0;36mget_IDAC_df\u001b[1;34m()\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_IDAC_df\u001b[39m():\n\u001b[0;32m    369\u001b[0m     \u001b[39m# rewrite with os.path.join\u001b[39;00m\n\u001b[0;32m    370\u001b[0m     \u001b[39m# IDAC_dir = r'C:\\Users\\efons\\Desktop\\ML\\VT_Database\\IDAC'\u001b[39;00m\n\u001b[0;32m    371\u001b[0m     IDAC_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(\u001b[39m__file__\u001b[39m)), \u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnew_data_parquet\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 372\u001b[0m     ddf \u001b[39m=\u001b[39m dd\u001b[39m.\u001b[39;49mread_parquet(IDAC_dir)\n\u001b[0;32m    373\u001b[0m     \u001b[39mreturn\u001b[39;00m ddf\n",
      "File \u001b[1;32mc:\\Users\\efons\\anaconda3\\envs\\venv\\lib\\site-packages\\dask\\backends.py:127\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    126\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mtype\u001b[39m(e)(\n\u001b[0;32m    128\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling the \u001b[39m\u001b[39m{\u001b[39;00mfuncname(func)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    129\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmethod registered to the \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackend\u001b[39m}\u001b[39;00m\u001b[39m backend.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    130\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOriginal Message: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    131\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: c:/Users/efons/anaconda3/envs/venv/lib/site-packages/cosmo_tl-0.0.1-py3.10.egg/COSMO_TL/data/new_data_parquet"
     ]
    }
   ],
   "source": [
    "ctl.GammaLoaders.get_IDAC_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources as pkg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_train_50.pickle']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkg.resource_listdir('COSMO_TL', 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import COSMO_TL as ctl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ctl.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = models.Conv(layers=3, layer_size=100, kernel_size=10, out_channels=(3, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: c:/Users/efons/anaconda3/envs/venv/lib/site-packages/COSMO_TL/data/new_data_parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\efons\\anaconda3\\envs\\venv\\lib\\site-packages\\dask\\backends.py:125\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 125\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    126\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\efons\\anaconda3\\envs\\venv\\lib\\site-packages\\dask\\dataframe\\io\\parquet\\core.py:479\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, columns, filters, categories, index, storage_options, engine, use_nullable_dtypes, calculate_divisions, ignore_metadata_file, metadata_task_size, split_row_groups, chunksize, aggregate_files, parquet_file_extension, **kwargs)\u001b[0m\n\u001b[0;32m    477\u001b[0m     index \u001b[39m=\u001b[39m [index]\n\u001b[1;32m--> 479\u001b[0m read_metadata_result \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39mread_metadata(\n\u001b[0;32m    480\u001b[0m     fs,\n\u001b[0;32m    481\u001b[0m     paths,\n\u001b[0;32m    482\u001b[0m     categories\u001b[39m=\u001b[39mcategories,\n\u001b[0;32m    483\u001b[0m     index\u001b[39m=\u001b[39mindex,\n\u001b[0;32m    484\u001b[0m     use_nullable_dtypes\u001b[39m=\u001b[39muse_nullable_dtypes,\n\u001b[0;32m    485\u001b[0m     gather_statistics\u001b[39m=\u001b[39mcalculate_divisions,\n\u001b[0;32m    486\u001b[0m     filters\u001b[39m=\u001b[39mfilters,\n\u001b[0;32m    487\u001b[0m     split_row_groups\u001b[39m=\u001b[39msplit_row_groups,\n\u001b[0;32m    488\u001b[0m     chunksize\u001b[39m=\u001b[39mchunksize,\n\u001b[0;32m    489\u001b[0m     aggregate_files\u001b[39m=\u001b[39maggregate_files,\n\u001b[0;32m    490\u001b[0m     ignore_metadata_file\u001b[39m=\u001b[39mignore_metadata_file,\n\u001b[0;32m    491\u001b[0m     metadata_task_size\u001b[39m=\u001b[39mmetadata_task_size,\n\u001b[0;32m    492\u001b[0m     parquet_file_extension\u001b[39m=\u001b[39mparquet_file_extension,\n\u001b[0;32m    493\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    494\u001b[0m )\n\u001b[0;32m    496\u001b[0m \u001b[39m# In the future, we may want to give the engine the\u001b[39;00m\n\u001b[0;32m    497\u001b[0m \u001b[39m# option to return a dedicated element for `common_kwargs`.\u001b[39;00m\n\u001b[0;32m    498\u001b[0m \u001b[39m# However, to avoid breaking the API, we just embed this\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[39m# data in the first element of `parts` for now.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m \u001b[39m# The logic below is inteded to handle backward and forward\u001b[39;00m\n\u001b[0;32m    501\u001b[0m \u001b[39m# compatibility with a user-defined engine.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\efons\\anaconda3\\envs\\venv\\lib\\site-packages\\dask\\dataframe\\io\\parquet\\arrow.py:355\u001b[0m, in \u001b[0;36mArrowDatasetEngine.read_metadata\u001b[1;34m(cls, fs, paths, categories, index, use_nullable_dtypes, gather_statistics, filters, split_row_groups, chunksize, aggregate_files, ignore_metadata_file, metadata_task_size, parquet_file_extension, **kwargs)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    336\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_metadata\u001b[39m(\n\u001b[0;32m    337\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    353\u001b[0m \n\u001b[0;32m    354\u001b[0m     \u001b[39m# Stage 1: Collect general dataset information\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m     dataset_info \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_collect_dataset_info(\n\u001b[0;32m    356\u001b[0m         paths,\n\u001b[0;32m    357\u001b[0m         fs,\n\u001b[0;32m    358\u001b[0m         categories,\n\u001b[0;32m    359\u001b[0m         index,\n\u001b[0;32m    360\u001b[0m         gather_statistics,\n\u001b[0;32m    361\u001b[0m         filters,\n\u001b[0;32m    362\u001b[0m         split_row_groups,\n\u001b[0;32m    363\u001b[0m         chunksize,\n\u001b[0;32m    364\u001b[0m         aggregate_files,\n\u001b[0;32m    365\u001b[0m         ignore_metadata_file,\n\u001b[0;32m    366\u001b[0m         metadata_task_size,\n\u001b[0;32m    367\u001b[0m         parquet_file_extension,\n\u001b[0;32m    368\u001b[0m         kwargs,\n\u001b[0;32m    369\u001b[0m     )\n\u001b[0;32m    371\u001b[0m     \u001b[39m# Stage 2: Generate output `meta`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\efons\\anaconda3\\envs\\venv\\lib\\site-packages\\dask\\dataframe\\io\\parquet\\arrow.py:874\u001b[0m, in \u001b[0;36mArrowDatasetEngine._collect_dataset_info\u001b[1;34m(cls, paths, fs, categories, index, gather_statistics, filters, split_row_groups, chunksize, aggregate_files, ignore_metadata_file, metadata_task_size, parquet_file_extension, kwargs)\u001b[0m\n\u001b[0;32m    873\u001b[0m \u001b[39mif\u001b[39;00m ds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 874\u001b[0m     ds \u001b[39m=\u001b[39m pa_ds\u001b[39m.\u001b[39mdataset(\n\u001b[0;32m    875\u001b[0m         paths,\n\u001b[0;32m    876\u001b[0m         filesystem\u001b[39m=\u001b[39mfs,\n\u001b[0;32m    877\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_dataset_kwargs,\n\u001b[0;32m    878\u001b[0m     )\n\u001b[0;32m    880\u001b[0m \u001b[39m# Deal with directory partitioning\u001b[39;00m\n\u001b[0;32m    881\u001b[0m \u001b[39m# Get all partition keys (without filters) to populate partition_obj\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\efons\\anaconda3\\envs\\venv\\lib\\site-packages\\pyarrow\\dataset.py:755\u001b[0m, in \u001b[0;36mdataset\u001b[1;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[0;32m    754\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(_is_path_like(elem) \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m source):\n\u001b[1;32m--> 755\u001b[0m     \u001b[39mreturn\u001b[39;00m _filesystem_dataset(source, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    756\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(elem, Dataset) \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m source):\n",
      "File \u001b[1;32mc:\\Users\\efons\\anaconda3\\envs\\venv\\lib\\site-packages\\pyarrow\\dataset.py:442\u001b[0m, in \u001b[0;36m_filesystem_dataset\u001b[1;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(source, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m--> 442\u001b[0m     fs, paths_or_selector \u001b[39m=\u001b[39m _ensure_multiple_sources(source, filesystem)\n\u001b[0;32m    443\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\efons\\anaconda3\\envs\\venv\\lib\\site-packages\\pyarrow\\dataset.py:361\u001b[0m, in \u001b[0;36m_ensure_multiple_sources\u001b[1;34m(paths, filesystem)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[39melif\u001b[39;00m file_type \u001b[39m==\u001b[39m FileType\u001b[39m.\u001b[39mNotFound:\n\u001b[1;32m--> 361\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(info\u001b[39m.\u001b[39mpath)\n\u001b[0;32m    362\u001b[0m \u001b[39melif\u001b[39;00m file_type \u001b[39m==\u001b[39m FileType\u001b[39m.\u001b[39mDirectory:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: c:/Users/efons/anaconda3/envs/venv/lib/site-packages/COSMO_TL/data/new_data_parquet",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# get data from the COSMO_TL package\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data \u001b[39m=\u001b[39m ctl\u001b[39m.\u001b[39;49mGammaLoaders\u001b[39m.\u001b[39;49mget_IDAC_df()\n\u001b[0;32m      3\u001b[0m data\n",
      "File \u001b[1;32mc:\\Users\\efons\\anaconda3\\envs\\venv\\lib\\site-packages\\COSMO_TL\\GammaLoaders.py:372\u001b[0m, in \u001b[0;36mget_IDAC_df\u001b[1;34m()\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_IDAC_df\u001b[39m():\n\u001b[0;32m    369\u001b[0m     \u001b[39m# rewrite with os.path.join\u001b[39;00m\n\u001b[0;32m    370\u001b[0m     \u001b[39m# IDAC_dir = r'C:\\Users\\efons\\Desktop\\ML\\VT_Database\\IDAC'\u001b[39;00m\n\u001b[0;32m    371\u001b[0m     IDAC_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(\u001b[39m__file__\u001b[39m)), \u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnew_data_parquet\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 372\u001b[0m     ddf \u001b[39m=\u001b[39m dd\u001b[39m.\u001b[39;49mread_parquet(IDAC_dir)\n\u001b[0;32m    373\u001b[0m     \u001b[39mreturn\u001b[39;00m ddf\n",
      "File \u001b[1;32mc:\\Users\\efons\\anaconda3\\envs\\venv\\lib\\site-packages\\dask\\backends.py:127\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    126\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mtype\u001b[39m(e)(\n\u001b[0;32m    128\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling the \u001b[39m\u001b[39m{\u001b[39;00mfuncname(func)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    129\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmethod registered to the \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackend\u001b[39m}\u001b[39;00m\u001b[39m backend.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    130\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOriginal Message: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    131\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: c:/Users/efons/anaconda3/envs/venv/lib/site-packages/COSMO_TL/data/new_data_parquet"
     ]
    }
   ],
   "source": [
    "# get data from the COSMO_TL package\n",
    "data = ctl.GammaLoaders.get_IDAC_df()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2c1d3e12e66dc47a0aae61db106152a68be04014ca291cb297471e9102f95b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
